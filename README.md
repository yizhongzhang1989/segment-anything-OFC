# SAM Finetuning for DLO Segmentation

Cloned from the original repo: https://github.com/facebookresearch/segment-anything


## Training Configuration

- A foreground dataset should be arranged with the following structure (both for test and train):

  ```
  FOREGROUND_DATASET_DIR
  ├── imgs
  |   ├── image_name_1.png
  |   ├── image_name_2.png
  |   └── ...
  └── masks
      ├── image_name_1.png
      ├── image_name_2.png
      └── ...
  ```

  Folder "imgs" contains RGB foreground images and folder "masks" contains the binary segmentation ground truth mask for those RGB images. The images can be of .png or .jpg format, and they should have consistent names in both the "imgs" and "masks" folders.

- A background dataset should be arranged with the following structure, containing RGB images:
  
  ```
  BACKGROUND_DATASET_DIR
  ├── image_name_1.png
  ├── image_name_2.png
  └── ...
  ```

The final dataset used for training is generated by cropping the foreground dataset according to the ground truth mask and superimposing it onto the background dataset. If no background dataset is specified, then the foreground dataset is used directly for training.


The configuration file describes the dataset used for training. An example is shown below:

```yaml
dataset:
  train:
    # A list of training datasets. 
    # They will be mixed by a certain percentage for training

    - data_dir: path/to/foreground_dataset  # FOREGROUND_DATASET_DIR
      
      # All data sources for background dataset.
      # This field may not exist.
      bg_paths:
        - path/to/background_images_1       # BACKGROUND_DATASET_DIR (1)
        - path/to/background_images_2       # BACKGROUND_DATASET_DIR (2)
        - # ...

      # Whether to add data augmentation to cable images and backgrounds.
      w_aug: true        
      
      # Data augmentation details.
      # Currently only support "crop_foreground", 
      # i.e. crop the foreground dataset 
      # and superimposing it onto the background dataset
      # This field may not exist.
      aug_details:
        - crop_foreground
      
      # In one epoch, what proportion of the images 
      # from this dataset will be used for training
      percentage: 0.1
    
    - # ...

  # The format of test dataset is similar to the train dataset described above, 
  # but it can only contain one data source.
  test:
    data_dir: path/to/test_dataset
    bg_paths: null
    w_aug: false
  
```

## Start Finetuning
After a config file is prepared, run the following command to start finetuning:

```bash
python finetune.py 
  --lr 1e-4 
  --epoch_num 500 
  --batch_size 16 
  --save_interval 50 
  --test_interval 100 
  --exp_name EXPERIMENT_NAME
  --config path/to/config_file
  --alpha 0.7
```

You can use ```--help``` to see the usage for the command line arguments. Here is a brief introduction:

- ```--config``` or ```-c```: Path to the config file described in the above section.
- ```--lr```: Initial learning rate (maximum). The learning rate will linearly increase to this maximum value and decay in the form of a power function.
- ```--epoch_num``` or ```-e```: The total number of training epochs.
- ```--batch_size``` or ```-b```: Batch size.
- ```--virtual_batch_size```: Used when the GPU memory is limited. The actual batch size = batch_size * virtual_batch_size. Determines the gradient clearing interval.
- ```--save_interval``` or ```-s```: Checkpoint save interval (num of epoches).
- ```--test_interval```: Test interval (num of iterations). Test dataset must be specified in the config file.
- ```--alpha```: Alpha value (0, 1) for focal loss.
- ```--exp_name``` or ```-n```: The name of the output folder. Checkpoints and training logs will be saved at "exp/exp_name".